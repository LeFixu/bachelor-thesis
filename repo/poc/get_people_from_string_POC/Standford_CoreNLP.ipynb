{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standfort CoreNLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/stanfordnlp/stanfordnlp\n",
    "https://stackoverflow.com/questions/39410282/coreference-resolution-in-python-nltk-using-stanford-corenlp\n",
    "\n",
    "\n",
    "Einmalig vorher in Python console : `pip install stanfordnlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1 = f'Meyer entgegnet: «Dieser Unterschied macht gerade dann viel aus, wenn eine Bank hopsgeht. Gerade deshalb ist es wichtig, wenn eine Bank auch einen grossen Teil vom Risiko selbst trägt.» Weiter sagt sie: «Gerade, weil Ex-CS-Chef Urs Rohner wusste, dass er ein hohes Risiko eingehen kann – und seine Bank im schlimmsten Fall vom Staat gerettet wird, dann bin ich fein raus.»'\n",
    "\n",
    "raw_text2 = \"\"\"\n",
    "Dass das Rennen am Sonntag äusserst knapp ausfallen würde, zeigten schon die Umfragewerte im Vorfeld: Gleich drei Parteien lagen fast gleichauf. Die konservative Nationale Sammlungspartei führte nur minimal vor der rechtspopulistischen Partei Die Finnen und vor Sanna Marins Sozialdemokraten. \n",
    "Bereits bei den letzten Wahlen 2019 lagen die drei Parteien nur weniger als einen Prozentpunkt auseinander. Damals konnten die Sozialdemokraten den Sieg für sich entscheiden, dieses Jahr zogen sie den Kürzeren.\n",
    "Dies, obwohl sie gegenüber 2019 mehr Stimmen holten und dadurch drei neue Parlamentssitze gewannen.  Eine Tatsache, über die sich Sanna Marin trotz der Niederlage freute. Das Problem: Den anderen zwei Parteien gelang das auch – und zwar noch ein Spürchen besser.\n",
    "Die Nationale Sammlungspartei unter der Führung des früheren Finanzministers Petteri Orpo holte mit 48 der 200 Sitze einen knappen Wahlsieg. Die Finnen-Partei sicherte sich 46 Sitze, die Sozialdemokraten kamen auf 43.\n",
    "\"\"\"\n",
    "\n",
    "raw_text3 = \"\"\"\n",
    "Sie war von 2017 bis 2020 Frankreichs erste Ministerin für die Gleichheit zwischen Frauen und Männern. Dank ihr sind sexistische Beleidigungen in Frankreich strafbar. Seit ihrer Amtszeit zählt Sex mit Jugendlichen unter 16 Jahren in Frankreich vor Gericht als Vergewaltigung. Und sie ist nun wohl die erste französische Politikerin auf dem Cover des französischen «Playboy».\n",
    "Die Rede ist von der Feministin und Staatssekretärin Marlène Schiappa. Derzeit ist sie in dem Land die Staatsministerin für Staatsbürgerschaft. Dass sie das Cover des Magazins ziert, sorgt für Furore. Vor allem bei politischen Gegner:innen und Kolleg:innen stossen ihre Fotos auf Kritik – und das, obwohl Schiappa darauf nicht einmal hüllenlos abgelichtet ist.\n",
    "In dem dazugehörigen Titel-Interview sprach die 40-Jährige laut Medienberichten auf zwölf Seiten über die Rechte von Frauen und LGBTQ-Themen. Worüber Frankreich nun spricht, ist aber nicht der Inhalt des Interviews, sondern eben doch der tiefe Ausschnitt, den Schiappas Kleid gehabt haben soll – und der Zeitpunkt der Veröffentlichung.\n",
    "«Playboy»-Titel stösst vor allem wegen Rentenreform sauer auf\n",
    "So soll Premierministerin Élisabeth Borne laut Medienberichten am Wochenende mit der Staatssekretärin über das Thema gesprochen haben. Oder, wie der Spiegel den Sender BFMTV zitiert, eher geschimpft. Sie bezeichnete das Interview demnach als «nicht angemessen». Vor allem wegen des Zeitpunktes.\n",
    "Damit spielte Borne wohl auf die landesweiten Unruhen wegen der umstrittenen Rentenreform an. Eine Grünen-Politikerin vermutete hinter der «Playboy»-Story sogar ein Ablenkungsmanöver. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"de_gsd\" for language \"de\".\n",
      "Would you like to download the models for: de_gsd now? (Y/n)\n",
      "\n",
      "Default download directory: C:\\Users\\anina\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: de_gsd\n",
      "Download location: C:\\Users\\anina\\stanfordnlp_resources\\de_gsd_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229M/229M [00:42<00:00, 5.37MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\anina\\stanfordnlp_resources\\de_gsd_models.zip\n",
      "Extracting models file for: de_gsd\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\anina\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Cannot load model from C:\\Users\\anina\\stanfordnlp_resources\\en_ewt_models\\en_ewt_tokenizer.pt\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:82\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(filename, \u001b[39mlambda\u001b[39;49;00m storage, loc: storage)\n\u001b[0;32m     83\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anina\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m stanfordnlp\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mde\u001b[39m\u001b[39m'\u001b[39m, force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m nlp \u001b[39m=\u001b[39m stanfordnlp\u001b[39m.\u001b[39;49mPipeline()\n\u001b[0;32m      7\u001b[0m doc \u001b[39m=\u001b[39m nlp(raw_text1)\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\pipeline\\core.py:121\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[0;32m    122\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    123\u001b[0m                                                                               use_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_gpu)\n\u001b[0;32m    124\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    125\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\pipeline\\processor.py:102\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, use_gpu)\n\u001b[0;32m    103\u001b[0m \u001b[39m# run set up process\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\pipeline\\tokenize_processor.py:31\u001b[0m, in \u001b[0;36mTokenizeProcessor._set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m Trainer(model_file\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmodel_path\u001b[39;49m\u001b[39m'\u001b[39;49m], use_cuda\u001b[39m=\u001b[39;49muse_gpu)\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:16\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, args, vocab, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[39m# load everything from file\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# build model from scratch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:85\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCannot load model from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(filename))\n\u001b[1;32m---> 85\u001b[0m     sys\u001b[39m.\u001b[39;49mexit(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2042\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2039\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[0;32m   2040\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2041\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 2042\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[0;32m   2043\u001b[0m                                                      value))\n\u001b[0;32m   2044\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2045\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2046\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2047\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2048\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[0;32m    578\u001b[0m     \u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \n\u001b[0;32m    580\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:452\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    449\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    450\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    451\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 452\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m    453\u001b[0m             etype, evalue, (etb, chained_exc_ids),\n\u001b[0;32m    454\u001b[0m             chained_exceptions_tb_offset, context)\n\u001b[0;32m    455\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[0;32m    456\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m tb\n\u001b[1;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1119\u001b[0m     \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1009\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[0;32m   1011\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1013\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1015\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[0;32m    857\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    858\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    862\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m    863\u001b[0m ):\n\u001b[0;32m    864\u001b[0m     \u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m    866\u001b[0m                                                            tb_offset)\n\u001b[0;32m    868\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[0;32m    797\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(etype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[0;32m    798\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m    803\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\IPython\\core\\ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    848\u001b[0m     formatter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    849\u001b[0m options \u001b[39m=\u001b[39m stack_data\u001b[39m.\u001b[39mOptions(\n\u001b[0;32m    850\u001b[0m     before\u001b[39m=\u001b[39mbefore,\n\u001b[0;32m    851\u001b[0m     after\u001b[39m=\u001b[39mafter,\n\u001b[0;32m    852\u001b[0m     pygments_formatter\u001b[39m=\u001b[39mformatter,\n\u001b[0;32m    853\u001b[0m )\n\u001b[1;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(stack_data\u001b[39m.\u001b[39;49mFrameInfo\u001b[39m.\u001b[39;49mstack_data(etb, options\u001b[39m=\u001b[39;49moptions))[tb_offset:]\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stack_data\\core.py:546\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[1;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_data\u001b[39m(\n\u001b[0;32m    532\u001b[0m         \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m         collapse_repeated_frames: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    537\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Union[\u001b[39m'\u001b[39m\u001b[39mFrameInfo\u001b[39m\u001b[39m'\u001b[39m, RepeatedFrames]]:\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     stack \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(iter_stack(frame_or_tb))\n\u001b[0;32m    548\u001b[0m     \u001b[39m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[39m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     \u001b[39m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stack_data\\utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mwhile\u001b[39;00m frame_or_tb:\n\u001b[0;32m     97\u001b[0m     \u001b[39myield\u001b[39;00m frame_or_tb\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n\u001b[0;32m     99\u001b[0m         frame_or_tb \u001b[39m=\u001b[39m frame_or_tb\u001b[39m.\u001b[39mf_back\n\u001b[0;32m    100\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stack_data\\utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     assert_(\u001b[39misinstance\u001b[39;49m(frame_or_tb, (types\u001b[39m.\u001b[39;49mFrameType, types\u001b[39m.\u001b[39;49mTracebackType)))\n\u001b[0;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(frame_or_tb, (types\u001b[39m.\u001b[39mFrameType,))\n",
      "File \u001b[1;32mc:\\Users\\anina\\anaconda3\\envs\\Project2\\lib\\site-packages\\stack_data\\utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[1;34m(condition, error)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    171\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mAssertionError\u001b[39;00m(error)\n\u001b[1;32m--> 172\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "stanfordnlp.download('de', force=True)\n",
    "\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "\n",
    "doc = nlp(raw_text1)\n",
    "\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coref: \n",
      "0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "1: Ministerin(9), ihr(19)\n",
      "2: Beleidigungen(22), ihrer(28)\n",
      "3: Amtszeit(29), sie(45)\n",
      "4: Rede(64), sie(76), sie(86), ihre(109)\n",
      "5: Fotos(110), darauf(119)\n",
      "6: BFMTV(216), Sie(222)\n",
      "7: Zeitpunktes(237), Damit(240)\n",
      "8: Grünen-Politikerin(255), der(258)\n",
      "\n",
      "NER: \n",
      "Frankreichs LOC\n",
      "Frankreich LOC\n",
      "Sex PER\n",
      "Frankreich LOC\n",
      "französische MISC\n",
      "französischen MISC\n",
      "Marlène Schiappa. PER\n",
      "Schiappa PER\n",
      "Titel-Interview LOC\n",
      "Frankreich LOC\n",
      "Schiappas PER\n",
      "Veröffentlichung. MISC\n",
      "Élisabeth Borne PER\n",
      "Spiegel ORG\n",
      "BFMTV ORG\n",
      "Zeitpunktes. MISC\n",
      "Borne PER\n",
      "Playboy»-Story MISC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nlp.add_pipe('coreferee')\n",
    "\n",
    "\n",
    "result= nlp(raw_text3)\n",
    "\n",
    "print(f'coref: ')\n",
    "result._.coref_chains.print()\n",
    "print('')\n",
    "print(f'NER: ')\n",
    "for i in result.ents:\n",
    "        print(i.text,i.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('$(', '$,', '$.', 'ADJA', 'ADJD', 'ADV', 'APPO', 'APPR', 'APPRART', 'APZR', 'ART', 'CARD', 'FM', 'ITJ', 'KOKOM', 'KON', 'KOUI', 'KOUS', 'NE', 'NN', 'NNE', 'PDAT', 'PDS', 'PIAT', 'PIS', 'PPER', 'PPOSAT', 'PPOSS', 'PRELAT', 'PRELS', 'PRF', 'PROAV', 'PTKA', 'PTKANT', 'PTKNEG', 'PTKVZ', 'PTKZU', 'PWAT', 'PWAV', 'PWS', 'TRUNC', 'VAFIN', 'VAIMP', 'VAINF', 'VAPP', 'VMFIN', 'VMINF', 'VMPP', 'VVFIN', 'VVIMP', 'VVINF', 'VVIZU', 'VVPP', 'XY')\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp.get_pipe(\"tagger\")\n",
    "print(tagger.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(lst):\n",
    "    \"\"\"\n",
    "    Remove duplicates from a list of string arrays\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for arr in lst:\n",
    "        if not any(set(arr) <= set(x) for x in result):\n",
    "            result.append(arr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sex'], ['Marlène', 'Schiappa'], ['Schiappas'], ['Élisabeth', 'Borne']]\n"
     ]
    }
   ],
   "source": [
    "# ner_people list\n",
    "\n",
    "# TODO merge list so that only one entry per person is in list\n",
    "\n",
    "ner_people_list = []\n",
    "for ent in result.ents:\n",
    "    if ent.label_ == 'PER':\n",
    "        clean_text = ent.text.replace(',', '').replace('.', '')\n",
    "        ner_people_list.append(clean_text.split(' '))\n",
    "\n",
    "ner_people_list\n",
    "\n",
    "# clean ner_people_list\n",
    "\n",
    "clean_ner_people_list = remove_duplicates(ner_people_list)\n",
    "\n",
    "print(clean_ner_people_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Frankreichs Frankreich Frankreich',\n",
       " ' Ministerin ihr',\n",
       " ' Beleidigungen ihrer',\n",
       " ' Amtszeit sie',\n",
       " ' Rede sie sie ihre',\n",
       " ' Fotos darauf',\n",
       " ' BFMTV Sie',\n",
       " ' Zeitpunktes Damit',\n",
       " ' Grünen-Politikerin der']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean coref list\n",
    "\n",
    "coref_chain = result._.coref_chains\n",
    "\n",
    "clean_coref_list = []\n",
    "\n",
    "for i in coref_chain:\n",
    "    line = i.pretty_representation\n",
    "    for c in line:\n",
    "        if c.isdigit() or c == '(' or c == ')' or c == ',' or c == ':':\n",
    "            line = line.replace(c, '')\n",
    "    \n",
    "    clean_coref_list.append(line)\n",
    "\n",
    "clean_coref_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gender_gap_tracker.classes.person import Person\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    salutation: list[str]       # Herr, Frau, Doktor, Prof.\n",
    "    pronouns: list[str]         # er, sie, thei, ihr, ihre, sein, seine\n",
    "    articles: list[str]         # der, die, das\n",
    "    substitute_nouns: list[str]  # Informatikerin, Forscherin, Studentin, Schwester, Tochter\n",
    "\n",
    "    #def __str__(self):\n",
    "    #    return f'first_name: {self.first_name}, last_name: {self.last_name}, pronouns: {self.pronouns}, substitute_nouns: {self.substitute_nouns}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person(first_name='Sex', last_name='', salutation=[], pronouns=[], articles=[], substitute_nouns=[])\n",
      "Person(first_name='Marlène', last_name='Schiappa', salutation=[], pronouns=[], articles=[], substitute_nouns=[])\n",
      "Person(first_name='Schiappas', last_name='', salutation=[], pronouns=[], articles=[], substitute_nouns=[])\n",
      "Person(first_name='Élisabeth', last_name='Borne', salutation=[], pronouns=[], articles=[], substitute_nouns=[])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get list of Person class\n",
    "\n",
    "people = []\n",
    "\n",
    "for person_words in clean_ner_people_list:\n",
    "    new_person = Person('','',[],[],[],[])\n",
    "    match = False\n",
    "    for pw in person_words:\n",
    "        for coref_entry in clean_coref_list:\n",
    "            if pw in coref_entry:\n",
    "                match = True\n",
    "                pos_res = nlp(coref_entry)\n",
    "                for w in pos_res:\n",
    "                    if w.pos_ == 'PROPN':\n",
    "                        new_person.first_name = w\n",
    "                    if w.pos_ == 'PRON':\n",
    "                        new_person.pro\n",
    "                        nouns.append(w)\n",
    "                    if w.pos_ == 'DET':\n",
    "                        new_person.pronouns.append(w)\n",
    "                    if w.pos_ == 'NOUN':\n",
    "                        new_person.substitute_nouns.append(w)\n",
    "    if(not match):\n",
    "        new_person.first_name = person_words[0]\n",
    "        if len(person_words) > 1:\n",
    "            new_person.last_name = person_words[1]\n",
    "    \n",
    "    people.append(new_person)\n",
    "\n",
    "for p in people:\n",
    "    print(p)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Frankreichs PROPN\n",
      "Frankreich PROPN\n",
      "Frankreich PROPN\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Ministerin NOUN\n",
      "ihr PRON\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Beleidigungen NOUN\n",
      "ihrer DET\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Amtszeit NOUN\n",
      "sie PRON\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Rede NOUN\n",
      "sie PRON\n",
      "sie PRON\n",
      "ihre DET\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Fotos NOUN\n",
      "darauf ADV\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "BFMTV PROPN\n",
      "Sie PRON\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Zeitpunktes NOUN\n",
      "Damit ADV\n",
      "finished line\n",
      "\n",
      "most specific: 0: Frankreichs(7), Frankreich(24), Frankreich(38)\n",
      "  SPACE\n",
      "Grünen-Politikerin NOUN\n",
      "der DET\n",
      "finished line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coref_chain = result._.coref_chains\n",
    "\n",
    "for i in coref_chain:\n",
    "    # print(f'most specific: {coref_chain[i.most_specific_mention_index].pretty_representation}')\n",
    "    line = i.pretty_representation\n",
    "    for c in line:\n",
    "        if c.isdigit() or c == '(' or c == ')' or c == ',' or c == ':':\n",
    "            line = line.replace(c, '')\n",
    "    \n",
    "    pos_res = nlp(line)\n",
    "    for w in pos_res:\n",
    "        print(w, w.pos_)\n",
    "    print(f'finished line')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meyer]\n"
     ]
    }
   ],
   "source": [
    "print(result._.coref_chains.resolve(result[38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most specific: 0: Meyer(0), sie(38)\n"
     ]
    }
   ],
   "source": [
    "coref_chain = result._.coref_chains\n",
    "\n",
    "print(f'most specific: {coref_chain[coref_chain[1].most_specific_mention_index].pretty_representation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meyer PROPN sb\n",
      "entgegnet VERB ROOT\n",
      ": PUNCT punct\n",
      "« PUNCT punct\n",
      "Dieser DET nk\n",
      "Unterschied NOUN sb\n",
      "macht VERB oc\n",
      "gerade ADV mo\n",
      "dann ADV mo\n",
      "viel PRON oa\n",
      "aus ADP svp\n",
      ", PUNCT punct\n",
      "wenn SCONJ cp\n",
      "eine DET nk\n",
      "Bank NOUN sb\n",
      "hopsgeht VERB mo\n",
      ". PUNCT punct\n",
      "Gerade ADV mo\n",
      "deshalb ADV mo\n",
      "ist AUX ROOT\n",
      "es PRON sb\n",
      "wichtig ADV pd\n",
      ", PUNCT punct\n",
      "wenn SCONJ cp\n",
      "eine DET nk\n",
      "Bank NOUN sb\n",
      "auch ADV mo\n",
      "einen DET nk\n",
      "grossen ADJ nk\n",
      "Teil NOUN oa\n",
      "vom ADP mo\n",
      "Risiko NOUN nk\n",
      "selbst ADV mnr\n",
      "trägt VERB re\n",
      ". PUNCT punct\n",
      "» PUNCT punct\n",
      "Weiter ADV mo\n",
      "sagt VERB ROOT\n",
      "sie PRON sb\n",
      ": PUNCT punct\n",
      "« PUNCT punct\n",
      "Gerade ADV mo\n",
      ", PUNCT punct\n",
      "weil SCONJ cp\n",
      "Ex-CS-Chef NOUN sb\n",
      "Urs PROPN pnc\n",
      "Rohner PROPN nk\n",
      "wusste VERB oc\n",
      ", PUNCT punct\n",
      "dass SCONJ cp\n",
      "er PRON sb\n",
      "ein DET nk\n",
      "hohes ADJ nk\n",
      "Risiko NOUN oa\n",
      "eingehen VERB oc\n",
      "kann AUX re\n",
      "– PUNCT punct\n",
      "und CCONJ cd\n",
      "seine DET nk\n",
      "Bank NOUN sb\n",
      "im ADP mo\n",
      "schlimmsten ADJ nk\n",
      "Fall NOUN nk\n",
      "vom ADP sbp\n",
      "Staat NOUN nk\n",
      "gerettet VERB oc\n",
      "wird AUX cj\n",
      ", PUNCT punct\n",
      "dann ADV mo\n",
      "bin AUX oc\n",
      "ich PRON sb\n",
      "fein ADV mo\n",
      "raus ADV svp\n",
      ". PUNCT punct\n",
      "» PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in result:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
